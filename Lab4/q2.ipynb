{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10e422ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter, defaultdict\n",
    "import random\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4754f4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"../Assignment_1/tokenized_hi.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a36acd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 3263994 sentences.\n",
      "Total tokens for training: 61035115\n"
     ]
    }
   ],
   "source": [
    "sentences = df['sentences'].explode().str['tokens'].tolist()\n",
    "all_tokens = [token for sentence in sentences for token in sentence]\n",
    "print(f\"Successfully loaded {len(sentences)} sentences.\")\n",
    "print(f\"Total tokens for training: {len(all_tokens)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2fc854d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Randomly selected 1000 sentences for testing.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "random.seed(42) \n",
    "test_sentences = random.sample(sentences, 1000)\n",
    "print(f\"Randomly selected {len(test_sentences)} sentences for testing.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4fe960f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size (V): 558957\n",
      "Pre-computation complete.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unigram_counts = Counter(all_tokens)\n",
    "bigram_counts = Counter(zip(all_tokens, all_tokens[1:]))\n",
    "\n",
    "V = len(unigram_counts)\n",
    "print(f\"Vocabulary Size (V): {V}\")\n",
    "\n",
    "# Pre-compute T(w)->the number of unique token types that follow each word w\n",
    "#Token Type Smoothing\n",
    "following_types = defaultdict(set)\n",
    "for w1, w2 in bigram_counts:\n",
    "    following_types[w1].add(w2)\n",
    "T = {word: len(types) for word, types in following_types.items()}\n",
    "print(\"Pre-computation complete.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039355dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_add_one_prob(sentence, unigram_counts, bigram_counts, V):\n",
    "    #Add-One (Laplace) smoothing\n",
    "    log_prob = 0.0\n",
    "    for i in range(len(sentence) - 1):\n",
    "        w1, w2 = sentence[i], sentence[i+1]\n",
    "        bigram = (w1, w2)\n",
    "        \n",
    "        # Formula: P(w2 | w1) = (count(w1, w2) + 1) / (count(w1) + V)\n",
    "        numerator = bigram_counts.get(bigram, 0) + 1\n",
    "        denominator = unigram_counts.get(w1, 0) + V\n",
    "        \n",
    "        log_prob += np.log(numerator / denominator)\n",
    "        \n",
    "    return log_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9f263c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_add_k_prob(sentence, unigram_counts, bigram_counts, V, k):\n",
    "    #Add-K smoothing\n",
    "    log_prob = 0.0\n",
    "    for i in range(len(sentence) - 1):\n",
    "        w1, w2 = sentence[i], sentence[i+1]\n",
    "        bigram = (w1, w2)\n",
    "        \n",
    "        # Formula: P(w2 | w1) = (count(w1, w2) + k) / (count(w1) + k*V)\n",
    "        numerator = bigram_counts.get(bigram, 0) + k\n",
    "        denominator = unigram_counts.get(w1, 0) + k * V\n",
    "        \n",
    "        log_prob += np.log(numerator / denominator)\n",
    "        \n",
    "    return log_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa837cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_token_type_prob(sentence, unigram_counts, bigram_counts, T):\n",
    "    #Token Type\" smoothing.\n",
    "    #P(w2 | w1) = (count(w1, w2) + 1) / (count(w1) + T(w1))\n",
    "    #T(w1)->number of unique word types that follow w1.\n",
    "\n",
    "    log_prob = 0.0\n",
    "    for i in range(len(sentence) - 1):\n",
    "        w1, w2 = sentence[i], sentence[i+1]\n",
    "        bigram = (w1, w2)\n",
    "        \n",
    "        # Get count of unique followers for w1, default to V if w1 is unknown\n",
    "        num_following_types = T.get(w1, V)\n",
    "        \n",
    "        numerator = bigram_counts.get(bigram, 0) + 1\n",
    "        denominator = unigram_counts.get(w1, 0) + num_following_types\n",
    "        \n",
    "        # Avoid division by zero if a token somehow has 0 count and 0 followers\n",
    "        if denominator == 0:\n",
    "            continue\n",
    "\n",
    "        log_prob += np.log(numerator / denominator)\n",
    "        \n",
    "    return log_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd759d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_value = 0.1 \n",
    "\n",
    "results = []\n",
    "\n",
    "for sentence in test_sentences:\n",
    "    #sentence to short for bi-gram\n",
    "    if len(sentence) < 2:\n",
    "        continue\n",
    "    \n",
    "    prob_add_one = calculate_add_one_prob(sentence, unigram_counts, bigram_counts, V)\n",
    "    prob_add_k = calculate_add_k_prob(sentence, unigram_counts, bigram_counts, V, k_value)\n",
    "    prob_token_type = calculate_token_type_prob(sentence, unigram_counts, bigram_counts, T)\n",
    "    \n",
    "    results.append({\n",
    "        \"sentence\": \" \".join(sentence),\n",
    "        \"add_one_log_prob\": prob_add_one,\n",
    "        \"add_k_log_prob\": prob_add_k,\n",
    "        \"token_type_log_prob\": prob_token_type\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa5c2d59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(A higher log probability, closer to 0, indicates a better fit by the model)\n",
      "\n",
      "Sentence 1: \"इस वीडियो में तेजस्वी प्रकाश के हाथ में एक सूटकेस है और वो ' बंटी और बबली ' के गाने ' धड़क धड़क ' पर थिरकती नज़र आ रही हैं।\"\n",
      "Add-One Smoothing Log Prob:      -238.3993\n",
      "Add-K (k=0.1) Smoothing Log Prob: -198.5167\n",
      "Token Type Smoothing Log Prob:   -145.1653\n",
      "\n",
      "Sentence 2: \"भागलपुर में सोमवार की देर रात अपराधियों ने एक पिकअप वैन चालक को गोली मार दी।\"\n",
      "Add-One Smoothing Log Prob:      -108.9366\n",
      "Add-K (k=0.1) Smoothing Log Prob: -85.1254\n",
      "Token Type Smoothing Log Prob:   -60.5898\n",
      "\n",
      "Sentence 3: \"इसके अलावा किसी बड़े रसूखदार व्यक्ति का मोबाइल चोरी होता है तो उसे चोरी की धारा में दर्ज किया जाता है , लेकिन आम आदमी का मोबाइल चोरी भी होता है तो उसे केवल खोया पाया सेल और गुम होने की तहरीर पर ही दर्ज किया जाता है।\"\n",
      "Add-One Smoothing Log Prob:      -336.7135\n",
      "Add-K (k=0.1) Smoothing Log Prob: -269.9604\n",
      "Token Type Smoothing Log Prob:   -216.6214\n",
      "\n",
      "Sentence 4: \"उन्हें बताना चाहिए कि वह किस परंपरा और अखाड़े से संबंधित हैं।\"\n",
      "Add-One Smoothing Log Prob:      -94.3535\n",
      "Add-K (k=0.1) Smoothing Log Prob: -77.2188\n",
      "Token Type Smoothing Log Prob:   -59.1376\n",
      "\n",
      "Sentence 5: \"हर छात्र के लिए परिवहन की व्यवस्था रहेगी।\"\n",
      "Add-One Smoothing Log Prob:      -51.6443\n",
      "Add-K (k=0.1) Smoothing Log Prob: -42.2443\n",
      "Token Type Smoothing Log Prob:   -36.0883\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"(A higher log probability, closer to 0, indicates a better fit by the model)\\n\")\n",
    "\n",
    "#for first 5 sentences\n",
    "for i in range(5):\n",
    "    res = results[i]\n",
    "    print(f\"Sentence {i+1}: \\\"{res['sentence']}\\\"\")\n",
    "    print(f\"Add-One Smoothing Log Prob:      {res['add_one_log_prob']:.4f}\")\n",
    "    print(f\"Add-K (k={k_value}) Smoothing Log Prob: {res['add_k_log_prob']:.4f}\")\n",
    "    print(f\"Token Type Smoothing Log Prob:   {res['token_type_log_prob']:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a7873a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
