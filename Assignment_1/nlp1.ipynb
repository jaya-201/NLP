{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a82251dc-1b63-4523-97f6-9a3e58063f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "ds = load_dataset(path=\"ai4bharat/IndicCorpV2\", split= \"hin_Deva\", streaming=\"True\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5e913c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73332d82-011a-4860-b2a5-8d5817cb085a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_tokenizer(text):\n",
    "    sentence_split = re.compile(r'(?<=[।!?])\\s+|(?<=[.!?])\\s+')\n",
    "    sentences = sentence_split.split(text.strip())\n",
    "    return [s.strip() for s in sentences if s.strip()]\n",
    "\n",
    "def word_tokenizer(sentence):\n",
    "    word_pattern = re.compile(\n",
    "        r'[\\u0900-\\u097F]+|'\n",
    "        r'\\d+\\.\\d+|'\n",
    "        r'\\d+|'\n",
    "        r'[\\w\\.-]+@[\\w\\.-]+|'\n",
    "        r'\\w+://\\S+|'\n",
    "        r'[^\\s\\w]',\n",
    "        re.UNICODE\n",
    "    )\n",
    "    return word_pattern.findall(sentence)\n",
    "\n",
    "def tokenize_paragraph(paragraph):\n",
    "    sentences = sentence_tokenizer(paragraph)\n",
    "    return [word_tokenizer(sent) for sent in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84b57de5-88e7-490c-a43f-c38d17fce820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1: ['मैं', 'बाजार', 'गया।']\n",
      "Sentence 2: ['फिर', 'मैंने', 'खाना', 'खाया', '!']\n",
      "Sentence 3: ['?']\n",
      "Sentence 4: ['http://example.com']\n"
     ]
    }
   ],
   "source": [
    "text = \"मैं बाजार गया। फिर मैंने खाना खाया! What about you? Visit http://example.com\"\n",
    "tokenized = tokenize_paragraph(text)\n",
    "\n",
    "for i, sent in enumerate(tokenized, 1):\n",
    "    print(f\"Sentence {i}: {sent}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b58f7b3-54ee-4b0d-bbb9-b6983e212559",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanned 0 entries, written 1 paragraphs...\n",
      "Scanned 100 entries, written 51 paragraphs...\n",
      "Scanned 200 entries, written 101 paragraphs...\n",
      "Scanned 300 entries, written 151 paragraphs...\n",
      "Scanned 400 entries, written 201 paragraphs...\n",
      "Scanned 500 entries, written 251 paragraphs...\n",
      "Scanned 600 entries, written 301 paragraphs...\n",
      "Scanned 700 entries, written 351 paragraphs...\n",
      "Scanned 800 entries, written 401 paragraphs...\n",
      "Scanned 900 entries, written 451 paragraphs...\n",
      "Scanned 1000 entries, written 501 paragraphs...\n",
      "Scanned 1100 entries, written 551 paragraphs...\n",
      "Scanned 1200 entries, written 601 paragraphs...\n",
      "Scanned 1300 entries, written 651 paragraphs...\n",
      "Scanned 1400 entries, written 701 paragraphs...\n",
      "Scanned 1500 entries, written 751 paragraphs...\n",
      "Scanned 1600 entries, written 801 paragraphs...\n",
      "Scanned 1700 entries, written 851 paragraphs...\n",
      "Scanned 1800 entries, written 901 paragraphs...\n",
      "Scanned 1900 entries, written 951 paragraphs...\n",
      "\n",
      "✅ Done. Written 1000 paragraphs to tokenized_hi.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "paragraphs = []\n",
    "count_written = 0\n",
    "\n",
    "for i, item in enumerate(ds):\n",
    "    if count_written >= 1000:\n",
    "        break\n",
    "\n",
    "    text = item.get(\"text\", \"\").strip()\n",
    "    if not text:\n",
    "        continue\n",
    "\n",
    "    sentence_texts = sentence_tokenizer(text)\n",
    "    paragraph = {\n",
    "        \"paragraph_id\": count_written,\n",
    "        \"sentences\": []\n",
    "    }\n",
    "\n",
    "    for sent_text in sentence_texts:\n",
    "        tokens = word_tokenizer(sent_text)\n",
    "        if tokens:\n",
    "            paragraph[\"sentences\"].append({\n",
    "                \"text\": sent_text,\n",
    "                \"tokens\": tokens\n",
    "            })\n",
    "\n",
    "    if paragraph[\"sentences\"]:\n",
    "        paragraphs.append(paragraph)\n",
    "        count_written += 1\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        print(f\"Scanned {i} entries, written {count_written} paragraphs...\")\n",
    "\n",
    "# Save to JSON file\n",
    "output_file = \"tokenized_hi.json\"\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as fout:\n",
    "    json.dump(paragraphs, fout, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"\\nDone. Written {count_written} paragraphs to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68a4262",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
